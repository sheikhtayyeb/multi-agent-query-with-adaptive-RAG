# â—ˆ AdaptiveRAG â€” Multiagent Frontend

A production-grade dark UI for the **Adaptive RAG multiagent pipeline** with two API endpoints:
- `POST /save-data-vectordb` â€” ingest URLs into the vector store
- `POST /agentic-query`      â€” run the LangGraph agent graph and return answers

---

## ğŸ“ Folder Structure

```
Project_Agent/
â”‚
â”œâ”€â”€ logs/                           # Runtime log files
â”‚
â”œâ”€â”€ src/                            # All source code
â”‚   â”œâ”€â”€ db/                         # Vector DB persistence layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ save_doc_db.py          # SaveDocDB â€” embed & store docs
â”‚   â”‚
â”‚   â”œâ”€â”€ graphs/                     # LangGraph graph definitions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ graph_builder.py        # GraphBuilder â€” assembles the agent graph
â”‚   â”‚
â”‚   â”œâ”€â”€ llms/                       # LLM client wrappers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ llm_factory.py
â”‚   â”‚
â”‚   â”œâ”€â”€ nodes/                      # Individual graph nodes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ retrieve.py             # Vector DB retrieval
â”‚   â”‚   â”œâ”€â”€ grade_documents.py      # Relevance grader
â”‚   â”‚   â”œâ”€â”€ generate.py             # Answer generation
â”‚   â”‚   â”œâ”€â”€ rewrite_query.py        # Query rewriter
â”‚   â”‚   â””â”€â”€ web_search.py           # Web search fallback
â”‚   â”‚
â”‚   â”œâ”€â”€ states/                     # TypedDict state schemas
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ graph_state.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                      # Tool definitions (search, etc.)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ search_tool.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ui/                         # â† Frontend lives here
â”‚   â”‚   â”œâ”€â”€ index.html              # Main HTML shell (3 tabs)
â”‚   â”‚   â”œâ”€â”€ styles.css              # Dark industrial theme
â”‚   â”‚   â””â”€â”€ app.js                  # Tab nav, API calls, trace rendering
â”‚   â”‚
â”‚   â”œâ”€â”€ logger.py                   # Logging configuration
â”‚   â””â”€â”€ main.py                     # FastAPI route handlers
â”‚
â”œâ”€â”€ app.py                          # Entrypoint â€” uvicorn server launch
â”œâ”€â”€ .env                            # API keys, model names, DB path
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ uv.lock
â”œâ”€â”€ request.json                    # Sample request payloads
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Configure the API base URL

Open `src/ui/app.js` and set your FastAPI server address:

```js
// src/ui/app.js â€” line 7
const API_BASE = "http://localhost:8000"; // â† matches uvicorn in app.py
```

### 2. Enable CORS in FastAPI

Add this to `src/main.py` so the browser can reach the API:

```python
# src/main.py
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # tighten to your serve URL in production
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### 3. Serve the UI as a static mount (recommended)

Mount `src/ui/` directly from FastAPI â€” no separate server needed:

```python
# src/main.py
from fastapi.staticfiles import StaticFiles

app.mount("/ui", StaticFiles(directory="src/ui", html=True), name="ui")
# Then open: http://localhost:8000/ui
```

Or serve standalone with Python:

```bash
cd src/ui && python -m http.server 3000
# Then open: http://localhost:3000
```

### 4. (Optional) Add a health check endpoint

The status dot in the header pings `/health` every 15 seconds:

```python
# src/main.py
@app.get("/health")
async def health():
    return {"status": "ok"}
```

### 5. Start the backend

```bash
# From project root
python app.py
# or
uvicorn src.main:app --reload --port 8000
```

---

## ğŸ› Features

| Feature | Description |
|---|---|
| **Ingest Tab** | Add multiple URLs, configure chunk size / overlap / DB path, watch the 4-stage pipeline animate |
| **Query Tab** | Type a question, run the agent graph, see answer + source chips |
| **Agent Trace** | Real-time timeline of nodes the graph visited (retrieve â†’ grade â†’ rewrite â†’ generate) |
| **State Inspector** | Pretty-printed JSON of the full LangGraph state with syntax highlighting |
| **Logs Tab** | Timestamped event feed, filterable by INFO / SUCCESS / ERROR |
| **API Status** | Live ping dot â€” green when backend is reachable |
| **Keyboard shortcut** | `Ctrl+Enter` / `âŒ˜+Enter` submits the query |

---

## ğŸ”Œ Adapting to Your Graph State

The `inferNodeTrace()` function in `src/ui/app.js` maps LangGraph state keys to trace events.
Update it to match your actual state schema from `src/states/graph_state.py`:

```js
function inferNodeTrace(state) {
  const events = [];
  // Map your graph's node outputs to trace events
  if (state.documents)          events.push({ node: "retrieve",        status: "success", detail: "..." });
  if (state.filtered_docs)      events.push({ node: "grade_documents", status: "success", detail: "..." });
  if (state.rewritten_question) events.push({ node: "rewrite_query",   status: "success", detail: "..." });
  if (state.generation)         events.push({ node: "generate",        status: "success", detail: "..." });
  return events;
}
```

And update `renderAnswer()` to pull from your state's answer key:

```js
const text = state.generation || state.answer || state.output;
```

---

## ğŸ“¦ Dependencies

**Zero** â€” pure HTML + CSS + vanilla JS. No build step, no npm, no bundler.
Drop the 3 files into `src/ui/` and you're done.