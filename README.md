# ğŸ¤– Project_Agent â€” Adaptive RAG with Multi-Agent Query System

A production-ready **Adaptive RAG (Retrieval-Augmented Generation)** system powered by **LangGraph** multi-agent orchestration. Features intelligent query routing, document grading, hallucination detection, and automatic query transformation â€” with a sleek dark-themed web UI.

[![Deploy to GCP](https://img.shields.io/badge/Deploy-GCP%20Cloud%20Run-4285F4?logo=google-cloud)](https://cloud.google.com/run)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?logo=fastapi)](https://fastapi.tiangolo.com)
[![LangGraph](https://img.shields.io/badge/LangGraph-Multi--Agent-FF6B6B)](https://github.com/langchain-ai/langgraph)

---

## ğŸ¯ What It Does

This system uses a **multi-agent graph** to intelligently answer questions by:

1. **Routing** â€” Decides whether to search your vector DB or the web
2. **Retrieving** â€” Pulls relevant documents from FAISS vector store
3. **Grading** â€” Filters out irrelevant documents using LLM scoring
4. **Generating** â€” Produces answers grounded in retrieved context
5. **Validating** â€” Checks for hallucinations and answer quality
6. **Transforming** â€” Rewrites queries if initial retrieval fails

**Live Demo:** [https://multi-agent-query-with-rag-xxx.run.app/ui](https://multi-agent-query-with-rag-257612295763.us-central1.run.app/ui)

---

## ğŸ—ï¸ Architecture

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Route Question (LLM-based Router)                          â”‚
â”‚  â”œâ”€ "web_search"    â†’ Web Search Tool                       â”‚
â”‚  â””â”€ "vectorstore"   â†’ FAISS Retriever                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                               â–¼
  Web Search                      Retrieve Documents
        â”‚                               â”‚
        â”‚                               â–¼
        â”‚                        Grade Documents
        â”‚                               â”‚
        â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â–¼                      â–¼
        â”‚            All Relevant           Not Relevant
        â”‚                   â”‚                      â”‚
        â”‚                   â”‚                      â–¼
        â”‚                   â”‚              Transform Query
        â”‚                   â”‚                      â”‚
        â”‚                   â”‚                      â””â”€â”€â”€â”€â”€â”
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                            â–¼                            â”‚
                    Generate Answer â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Grade Answer Quality         â”‚
            â”œâ”€ Grounded? (Hallucination)   â”‚
            â”œâ”€ Useful? (Answers question)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼           â–¼              â–¼
   Not Grounded   Useful      Not Useful
        â”‚           â”‚              â”‚
     Regenerate     END      Transform Query
```

---

## ğŸ“ Project Structure

```
Project_Agent/
â”‚
â”œâ”€â”€ .github/                        # Optional: Add for CI/CD auto-deploy
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml              # GitHub Actions pipeline
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ doc_vectordb.py         # SaveDocDB: embed & persist docs in FAISS
â”‚   â”‚   â””â”€â”€ fiass_index/            # Vector DB storage (ephemeral on Cloud Run)
â”‚   â”‚
â”‚   â”œâ”€â”€ graphs/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ graph_builder.py        # GraphBuilder: assembles LangGraph
â”‚   â”‚
â”‚   â”œâ”€â”€ llms/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ llm.py                  # LLM: OpenAI + Groq model wrappers
â”‚   â”‚
â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ node.py                 # AgentNode: all graph node logic
â”‚   â”‚       â”œâ”€â”€ route_question()
â”‚   â”‚       â”œâ”€â”€ retriever()
â”‚   â”‚       â”œâ”€â”€ grade_documents()
â”‚   â”‚       â”œâ”€â”€ decide_to_generate()
â”‚   â”‚       â”œâ”€â”€ transform_query()
â”‚   â”‚       â”œâ”€â”€ generate()
â”‚   â”‚       â””â”€â”€ grade_generation_v_documents_and_question()
â”‚   â”‚
â”‚   â”œâ”€â”€ states/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ state.py                # GraphState + Pydantic schemas
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ tool.py                 # Tools: Tavily web search
â”‚   â”‚
â”‚   â”œâ”€â”€ ui/                         # Frontend (vanilla JS, no build)
â”‚   â”‚   â”œâ”€â”€ app.js                  # API calls, trace rendering
â”‚   â”‚   â”œâ”€â”€ index.html              # 3 tabs: Ingest, Query, Logs
â”‚   â”‚   â””â”€â”€ styles.css              # Dark industrial theme
â”‚   â”‚
â”‚   â”œâ”€â”€ logger.py                   # Logging configuration
â”‚   â””â”€â”€ main.py                     # FastAPI app + routes
â”‚
â”œâ”€â”€ .dockerignore                   # Docker build exclusions
â”œâ”€â”€ .env                            # API keys (local only, never committed)
â”œâ”€â”€ .gitignore                      # Git exclusions
â”œâ”€â”€ .python-version                 # Python version spec
â”œâ”€â”€ app.py                          # Uvicorn entrypoint
â”œâ”€â”€ Dockerfile                      # Container build instructions
â”œâ”€â”€ notebook.ipynb                  # Jupyter notebook (experiments/testing)
â”œâ”€â”€ pyproject.toml                  # Python project metadata
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ request.json                    # Sample API request payloads
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ uv.lock                         # UV package manager lock file
```

---

## ğŸš€ Quick Start (Local Development)

### 1. Clone & Install

```bash
git clone https://github.com/YOUR_USERNAME/project-agent.git
cd project-agent

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Set Up Environment Variables

Create `.env` file in project root:

```env
LANGCHAIN_API_KEY=your_langchain_key
LANGSMITH_API_KEY=your_langsmith_key
GROQ_API_KEY=your_groq_key
OPENAI_API_KEY=your_openai_key
tavily_search_api=your_tavily_key
```

### 3. Run the Server

```bash
python app.py
```

Open browser: **http://localhost:8000/ui**

---

## ğŸ“¡ API Endpoints

| Endpoint | Method | Description |
|---|---|---|
| `/ui` | GET | Web interface (3 tabs: Ingest, Query, Logs) |
| `/health` | GET | Health check for monitoring |
| `/save-data-vectordb` | POST | Ingest URLs â†’ embed â†’ store in FAISS |
| `/agentic-query` | POST | Run multi-agent graph and return answer |

### Example: Ingest Documents

```bash
curl -X POST http://localhost:8000/save-data-vectordb \
  -H "Content-Type: application/json" \
  -d '{
    "urls": ["https://example.com/doc1", "https://example.com/doc2"],
    "chunk_size": 500,
    "chunk_overlap": 50
  }'
```

### Example: Query

```bash
curl -X POST http://localhost:8000/agentic-query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is LangGraph?"}'
```

---

## ğŸ›ï¸ Frontend Features

The UI (`/ui`) provides a complete interface for the RAG pipeline:

### **Ingest Tab**
- Add multiple URLs dynamically
- Configure chunk size, overlap, and DB path
- Watch a 4-stage animated pipeline: Fetch â†’ Split â†’ Embed â†’ Index
- Real-time success/error states per step

### **Query Tab**
- Natural language question input
- Live **Agent Trace** timeline showing graph execution:
  - `retrieve` â†’ `grade_documents` â†’ `transform_query` â†’ `generate`
- **State Inspector** with syntax-highlighted JSON of full graph state
- Answer with source document chips
- Keyboard shortcut: `Ctrl+Enter` / `âŒ˜+Enter` to submit

### **Logs Tab**
- Timestamped event feed (INFO / SUCCESS / ERROR)
- Filterable by log level
- Real-time updates from both ingest and query pipelines

---

## ğŸ§  How the Agent Graph Works

### Node Descriptions

| Node | Purpose | Logic |
|---|---|---|
| **route_question** | Decides retrieval strategy | LLM classifies query â†’ `"web_search"` or `"vectorstore"` |
| **retriever** | Fetches from FAISS | Embeds query â†’ retrieves top-k similar docs |
| **grade_documents** | Filters irrelevant docs | LLM scores each doc: `"yes"` (keep) or `"no"` (discard) |
| **decide_to_generate** | Checks if docs are sufficient | If all docs filtered â†’ transform query; else â†’ generate |
| **transform_query** | Rewrites query for better retrieval | LLM rephrases question to improve semantic match |
| **generate** | Produces answer | RAG chain: context + question â†’ LLM â†’ answer |
| **grade_generation** | Validates answer quality | 2-step check: (1) Grounded in docs? (2) Answers question? |
| **web_search** | Fallback for out-of-domain queries | Tavily API â†’ fetches web results â†’ formats as context |

### Edge Conditions

```python
# START â†’ route_question
if route == "web_search":    â†’ web_search â†’ generate
if route == "vectorstore":   â†’ retrieve â†’ grade_documents

# grade_documents â†’ decide_to_generate
if all_docs_filtered:        â†’ transform_query â†’ retrieve (loop)
else:                        â†’ generate

# generate â†’ grade_generation
if not_grounded:             â†’ generate (retry)
if not_useful:               â†’ transform_query â†’ retrieve (loop)
if useful:                   â†’ END
```

### Adaptive Behavior Examples

**Query:** "What are the latest iPhone features?"
```
route_question â†’ "web_search" (not in vectorstore docs)
  â†’ web_search (Tavily API)
  â†’ generate
  â†’ grade_generation: "useful" â†’ END
```

**Query:** "Explain LangGraph conditional edges" (in docs)
```
route_question â†’ "vectorstore"
  â†’ retrieve (5 docs)
  â†’ grade_documents (3 relevant, 2 filtered)
  â†’ decide_to_generate: "generate"
  â†’ generate
  â†’ grade_generation:
      â”œâ”€ grounded? "yes"
      â””â”€ useful? "yes" â†’ END
```

**Query:** "What is XYZ?" (vague, retrieval fails)
```
route_question â†’ "vectorstore"
  â†’ retrieve (0 relevant docs after grading)
  â†’ decide_to_generate: "transform_query"
  â†’ transform_query ("What is XYZ?" â†’ "Explain XYZ concept in detail")
  â†’ retrieve (now finds relevant docs)
  â†’ grade_documents â†’ generate â†’ END
```

---

## â˜ï¸ Deployment (GCP Cloud Run)

### Prerequisites

- Google Cloud account with billing enabled
- `gcloud` CLI installed
- Docker installed

### One-Time Setup

```bash
# 1. Login & set project
gcloud auth login
gcloud config set project YOUR_PROJECT_ID

# 2. Enable services
gcloud services enable cloudbuild.googleapis.com run.googleapis.com

# 3. Build & push image
gcloud builds submit --tag gcr.io/YOUR_PROJECT_ID/project-agent

# 4. Deploy
gcloud run deploy project-agent \
  --image gcr.io/YOUR_PROJECT_ID/project-agent \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --port 8080 \
  --memory 2Gi \
  --cpu 2 \
  --set-env-vars "LANGCHAIN_API_KEY=xxx,LANGSMITH_API_KEY=xxx,GROQ_API_KEY=xxx,OPENAI_API_KEY=xxx,tavily_search_api=xxx"
```

### CI/CD with GitHub Actions

Every push to `main` auto-deploys via `.github/workflows/deploy.yml`.

**Setup once:**
1. Create GCP service account with Cloud Run Admin role
2. Download JSON key
3. Add to GitHub Secrets:
   - `GCP_PROJECT_ID`
   - `GCP_SA_KEY` (entire JSON key)
   - All API keys

**Then just:**
```bash
git add .
git commit -m "update"
git push origin main
# â†’ Auto-builds & deploys to Cloud Run âœ…
```

---

## ğŸ”§ Configuration

### Models Used

| Component | Model | Provider | Purpose |
|---|---|---|
| Routing | `qwen/qwen3-32b` | Groq | Fast structured output for query classification |
| Grading | `qwen/qwen3-32b` | Groq | Document relevance scoring |
| Generation | `qwen/qwen3-32b` | Groq | Answer synthesis (can swap to `gpt-4o-mini`) |
| Embeddings | `text-embedding-3-large` | OpenAI | 1024-dim vector embeddings |

### Customization Points

**Change LLM models** â€” edit `src/nodes/node.py`:
```python
self.agent_node = AgentNode(
    model_openai="gpt-4o",      # for generation
    model_groq="llama-3.1-70b"  # for routing/grading
)
```

**Adjust chunk size** â€” in frontend or API:
```python
# src/main.py
doc_splits = savedocdb.doc_splits(
    chunk_size=1000,      # default: 500
    chunk_overlap=100     # default: 50
)
```

**Change vector DB path**:
```python
db_path = "./src/db/fiass_index"  # local or GCS bucket path
```

---

## ğŸ› Known Limitations & Roadmap

### Current Limitations

| Issue | Workaround |
|---|---|
| **FAISS index is ephemeral on Cloud Run** | Each deploy resets vector DB. Use GCS bucket for persistence. |
| **No user authentication** | Add OAuth via FastAPI middleware |
| **No query history** | Store queries in Cloud SQL or Firestore |
| **Single FAISS index** | No multi-tenancy support yet |

### Roadmap

- [ ] Persistent vector DB via GCS or Pinecone
- [ ] User authentication & query history
- [ ] Streaming LLM responses (SSE)
- [ ] Multi-index support for different document collections
- [ ] Agent trace export (JSON/CSV)
- [ ] Fine-tuned embeddings for domain-specific docs

---

## ğŸ“Š Monitoring & Logs

### Local Logs

```bash
# Tail logs in real-time
tail -f logs/app.log
```

### Cloud Run Logs

```bash
# View live logs
gcloud run logs tail project-agent --region us-central1

# Filter by severity
gcloud run logs read project-agent --region us-central1 --filter="severity=ERROR"
```

### LangSmith Tracing

Every graph execution is traced in LangSmith (if `LANGSMITH_API_KEY` is set):
- View graph execution flow
- Debug node inputs/outputs
- Measure latency per node
- Track token usage

Access: https://smith.langchain.com

---

## ğŸ› ï¸ Development

### Running Tests

```bash
# Install test dependencies
pip install pytest pytest-asyncio httpx

# Run tests
pytest tests/
```

### Linting & Formatting

```bash
pip install black ruff

# Format code
black src/

# Lint
ruff check src/
```

### Adding New Nodes

1. Add node function to `src/nodes/node.py`:
```python
def my_new_node(self, state: GraphState):
    # Your logic here
    return {"new_key": "value"}
```

2. Register in `src/graphs/graph_builder.py`:
```python
self.graph.add_node("my_node", self.agent_node.my_new_node)
self.graph.add_edge("previous_node", "my_node")
```

3. Update `GraphState` in `src/states/state.py` if adding new state keys

---

## ğŸ“š Tech Stack

| Category | Technology |
|---|---|
| **Framework** | FastAPI, Uvicorn |
| **Agent Orchestration** | LangGraph (StateGraph) |
| **LLMs** | OpenAI GPT-4o-mini, Groq (Qwen3-32B) |
| **Embeddings** | OpenAI `text-embedding-3-large` |
| **Vector DB** | FAISS (local) |
| **Web Search** | Tavily API |
| **Frontend** | Vanilla JS, CSS, HTML (zero build) |
| **Deployment** | GCP Cloud Run, Docker |
| **CI/CD** | GitHub Actions |
| **Monitoring** | LangSmith, Cloud Logging |

---


## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details

---

## ğŸ™ Acknowledgments

- [LangChain](https://langchain.com) for LangGraph framework
- [FastAPI](https://fastapi.tiangolo.com) for the web framework
- [Tavily](https://tavily.com) for web search API
- [Groq](https://groq.com) for blazing-fast LLM inference